{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad963dc5-3351-4331-bb2b-0d969aa07557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from transformers import *\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import timm\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c3329-d837-40b5-9519-368cd946b772",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f962d795-a991-44c3-840e-c79cee539da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"vit_base_patch16_384.orig_in21k_ft_in1k\"\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e704f23-35b0-49a1-8cad-6468c739813e",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "762f6f6f-2a6f-40d7-b090-002cd4665261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /home/moose/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d71cff2fc444c54a6c0ac29b332235b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/14462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform(examples):\n",
    "  inputs = image_processor([img.convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
    "  inputs[\"labels\"] = examples[\"label\"]\n",
    "  return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "  return {\n",
    "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
    "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
    "  }\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "batch_size = 16\n",
    "cpu_count=multiprocessing.cpu_count()\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "train_ds= load_dataset('../../../Desktop_SIH/SIH/Main_Dataset/')\n",
    "train_ds = train_ds[\"train\"].train_test_split(test_size=0.25) \n",
    "\n",
    "\n",
    "#train_ds= load_dataset('chest_xray/train/',num_proc=cpu_count)\n",
    "#test_ds= load_dataset('chest_xray/test/',num_proc=cpu_count)\n",
    "\n",
    "labels = train_ds[\"train\"].features[\"label\"].names\n",
    "#dataset_train = train_ds.with_transform(transform)\n",
    "#dataset_test=test_ds.with_transform(transform)\n",
    "dataset = train_ds.with_transform(transform)\n",
    "\n",
    "\n",
    "train_dataset_loader = torch.utils.data.DataLoader(dataset[\"train\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset_loader = torch.utils.data.DataLoader(dataset[\"test\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#train_dataset_loader = torch.utils.data.DataLoader(dataset_train[\"train\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "#valid_dataset_loader = torch.utils.data.DataLoader(dataset_test[\"train\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ac765-87fd-4631-b2d1-250c936a0248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
